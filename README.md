# openai-proxy-server
use litellm as an openai proxy server for redis caching and llm observability with langfuse
