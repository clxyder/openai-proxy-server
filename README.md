# openai-proxy-server

Use litellm as an openai proxy server for redis caching and llm observability with langfuse

Copy `.env.example` to `.env` and update missing environment variables.

Then, run the following command to start the proxy server:

```bash
docker compose up -d
```
